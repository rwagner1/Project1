In this model a number of $k$ ants is sent on a network of $m$ cities with every ant starting at the same city. The next ant only starts when the previous ant has finished its tour which means there never are two ants in the network. \\
\subsection{Choosing a City}
Before moving on to the next city an ant has to decide where it wants to go. For this purpose it chooses randomly a number $q$ between zero and one and if this number is smaller or equal than a certain parameter $q_0$ ($q \leqslant q_0$) it looks for the city $s$ which fulfils the following formula:
\begin{equation}
s = \text{arg max}\{[\tau (r,u)] \cdot [\eta (r,u)]^{\beta}\}, \hspace{2cm} u \not\in M_k.
\label{eq:qsmallerq0}
\end{equation}
The current city where the ant stays is denoted by $r$ and only cities can be chosen which are not yet in the ants memory $M_k$ which means the ant has not visited these cities. The matrix $\tau$ stores the information about the amount of pheromone on the edge between city $r$ and city $u$ and the function $\eta$ gives the inverse of the distance between the two cities. \\
If the random number is bigger than $q_0$ ($q > q_0$) the ant randomly chooses one of the remaining unvisited cities and accepts or rejects it according to the probability $p_k$:
\begin{equation}
p_k (r,s) = \frac{[\tau (r,s)] \cdot [\eta (r,s)]^{\beta}}{\sum_{u \not\in M_k}[\tau (r,u)] \cdot [\eta (r,u)]^{\beta}}.
\label{eq:prob}
\end{equation}
This probability basically contains the same formula of $\tau$ and $\eta$ as the one above but is now normalized with the sum over these relations of every unvisited city. One can clearly see that in the beginning the sum is big and the probabilities are small but favouring the edges with more pheromone and lower distance. At the end when only one city is left the sum equals the term in the nominator and the probability becomes one for the last remaining city.
\subsection{Moving Forward and Updating}
Once the city has been chosen the ant moves along the edge and the pheromone on the path is updated according to:
\begin{equation}
\tau (r,s) = (1-\alpha)\cdot \tau(r,s) + \alpha \tau_0.
\label{eq:loctauupdate}
\end{equation}
The newly introduced parameters $\alpha$ and $\tau_0$ are explained in chapter \ref{sec:results}. This update reduces the amount of pheromone on the chosen edge and helps avoiding very strong edges which would be taken by all the ants. \\
At the time the first ant has finished the tour the second one can start while the first ant still keeps in mind the trajectory of its tour which means the sequence of the city it has visited and the length of its tour but deletes its memory such that it is ready for a new tour. When all ants have completed one tour the shortest one is rewarded with pheromone according to the formula:
\begin{equation}
\tau(r,s) = (1-\alpha)\cdot \tau(r,s) + \alpha \Delta \tau(r,s).
\label{eq:globalupdate}
\end{equation}
This update is intended to give the edges along the shortest path a little head start in the following round. With this step the first round is complete and the second round can start.

\subsection{Modifications}

In the above model every ant started at city number 1 and waited for his predecessor to finish his tour completely before it started its own tour. In the paper [1] underlying this report the model was slightly different. All agents are placed randomly on any city in the network and move together. When the first agent chose his second city and moved there he waits until every ant has found its next city. So in every time step all agents move for one city. The difference to the model before is that every ant feels the pheromone on the edges of all the other ants in the network. Whereas in the previous model the second ant only saw the trace the first ant left but did not feel any influence of the still following ants. \\
Another small change which was tested is to increase the award for the shortest path which was found after one round. This means to add a constant amount of pheromone to each edge along the shortest path in the global update formula (equation \ref{eq:globalupdate}).

\begin{equation}
\tau(r,s) = (1-\alpha)\cdot \tau(r,s) + \alpha \Delta \tau(r,s) \textbf{+ 0.1}.
\label{eq:globalupdate2}
\end{equation}